{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/large-traversaal/Alif-1.0-8B-Instruct\n","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install -q gradio\n!pip install bitsandbytes","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T09:21:36.170584Z","iopub.execute_input":"2025-07-24T09:21:36.171181Z","iopub.status.idle":"2025-07-24T09:22:56.100459Z","shell.execute_reply.started":"2025-07-24T09:21:36.171161Z","shell.execute_reply":"2025-07-24T09:22:56.099355Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport gradio as gr\nfrom threading import Thread\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer, TextIteratorStreamer\n\n\nmodel_id = \"large-traversaal/Alif-1.0-8B-Instruct\"\n\n# 4-bit quantization configuration\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\n# Load tokenizer and model in 4-bit\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T09:23:36.462718Z","iopub.execute_input":"2025-07-24T09:23:36.463304Z","iopub.status.idle":"2025-07-24T09:27:28.829211Z","shell.execute_reply.started":"2025-07-24T09:23:36.463274Z","shell.execute_reply":"2025-07-24T09:27:28.828374Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 09:23:51.569436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753349031.768553      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753349031.830065      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c3c77b19cb94cc8a40b95ad0de1e24e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0390d75697f9482fbbfe5de7dd0bb571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9efe3de845594d02a69f39a7ffaecea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b2840b4af34d738915a8d267e88b89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8a9d75a1d448b9a87a96183686259f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/947 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6960fba8636c41ebb7594bee92bd7ca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179f02ac397b45968b77f8b90042654b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca85f50183774297b79f0dec12b2153a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1725b20c1f9c4541be740f3699d19760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a99105eb1141898a235d5750126fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3ac53f87cb47908033cfbd7581f415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b4c832fd634d238afb41704b60cbdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b18f5d31f5473ea9cf5bfa56d24a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca0c19e4333340158b0a78c85eb4b64a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"598816087f764cb9a0367e49f46621fe"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import gradio as gr\nimport torch\nfrom threading import Thread\nfrom transformers import TextIteratorStreamer\n\n\n# --- Main Change 1: Function to format the prompt with history ---\ndef format_prompt_with_history(message, history):\n    # The base instruction for the model\n    chat_prompt = \"\"\"You are Urdu Chatbot.\n### Instruction:\nBelow is an instruction that describes a task. Write a response in urdu that appropriately completes the request. Don't say you don't know unless you really don't.\nPlease do not give long answers unless specifically asked for it. Always try to answer in maximum of 2-3 lines.\n\"\"\"\n    \n    # Add past conversations to the prompt\n    for user_msg, bot_msg in history:\n        chat_prompt += f\"### Input:\\n{user_msg}\\n### Response:\\n{bot_msg}\\n\"\n        \n    # Add the current user message\n    chat_prompt += f\"### Input:\\n{message}\\n### Response:\\n\"\n    \n    return chat_prompt\n\n# --- Main Change 2: The generation function now accepts history ---\n# This function is now a generator to stream output token-by-token\ndef generate_response(message, history):\n    # Format the prompt to include the conversation history\n    prompt = format_prompt_with_history(message, history)\n    \n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    generation_kwargs = dict(\n        inputs,\n        streamer=streamer,\n        max_new_tokens=500,\n        do_sample=True,\n        top_p=0.95,\n        top_k=50,\n        temperature=0.7,\n        repetition_penalty=1.2,\n    )\n\n    # Run generation in a separate thread\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    # Yield new text as it's generated\n    buffer = \"\"\n    for new_text in streamer:\n        buffer += new_text\n        yield buffer\n\n# --- Main Change 3: Using gr.ChatInterface for a Gemini-like UI ---\niface = gr.ChatInterface(\n    fn=generate_response,\n    title=\"Urdu Chatbot ðŸ¤– (Alif-1.0-8B)\",\n    description=\"Aap Urdu mein kuch bhi pooch sakte hain. (Ask me anything in Urdu)\",\n    chatbot=gr.Chatbot(height=500), # Sets the height of the chat window\n    textbox=gr.Textbox(placeholder=\"...ÛŒÛØ§Úº Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ Ù„Ú©Ú¾ÛŒÚº\", container=False, scale=7),\n    examples = [\n        'Ø´ÛØ± Ú©Ø±Ø§Ú†ÛŒ Ú©ÛŒ Ú©ÛŒØ§ Ø§ÛÙ…ÛŒØª ÛÛ’ØŸ',\n        'ØµØ­Øª Ù…Ù†Ø¯ Ø±ÛÙ†Û’ Ú©Û’ Ù„Ø¦Û’ Ù¾Ø§Ù†Ú† ØªØ¬Ø§ÙˆÛŒØ² Ø¯ÛŒÚºÛ”',\n        'Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ ØªØ§Ø±ÛŒØ® Ù¾Ø± Ø§ÛŒÚ© Ù…Ø®ØªØµØ± Ù…Ø¶Ù…ÙˆÙ† Ù„Ú©Ú¾ÛŒÚºÛ”'\n    ],\n    # retry_btn=\"Retry ðŸ”\",\n    # undo_btn=\"Undo â†©ï¸\",\n    # clear_btn=\"Clear âœ¨\",\n)\n\niface.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T09:33:23.547615Z","iopub.execute_input":"2025-07-24T09:33:23.548266Z","iopub.status.idle":"2025-07-24T09:54:38.547615Z","shell.execute_reply.started":"2025-07-24T09:33:23.548242Z","shell.execute_reply":"2025-07-24T09:54:38.547011Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3165976019.py:61: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot=gr.Chatbot(height=500), # Sets the height of the chat window\n/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:322: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://ba870a536f94d40f05.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ba870a536f94d40f05.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://ba870a536f94d40f05.gradio.live\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}